(dp1
(Vfirst_crowler_qiubai
p2
S'pending'
tp3
ccopy_reg
_reconstructor
p4
(cpyspider.libs.counter
TotalCounter
p5
c__builtin__
object
p6
NtRp7
(dp8
S'cnt'
p9
I1
sbs(g2
S'success'
tp10
g4
(g5
g6
NtRp11
(dp12
g9
I0
sbs(g2
S'failed'
tp13
g4
(g5
g6
NtRp14
(dp15
g9
I0
sbs(g4
(cpyspider.scheduler.scheduler
Project
p16
g6
NtRp17
(dp18
S'updatetime'
p19
F1471677374.727
sS'waiting_get_info'
p20
I00
sS'group'
p21
NsS'name'
p22
Vfirst_crowler_qiubai
p23
sS'project_info'
p24
(dp25
S'status'
p26
VDEBUG
p27
sS'updatetime'
p28
F1471677374.727
sS'group'
p29
NsS'name'
p30
g23
sS'script'
p31
V#!/usr/bin/env python\u000a# -*- encoding: utf-8 -*-\u000a# Created on 2016-08-20 14:57:38\u000a# Project: first_crowler_qiubai\u000a\u000afrom pyspider.libs.base_handler import *\u000a\u000a\u000aclass Handler(BaseHandler):\u000a    crawl_config = {\u000a    }\u000a\u000a    @every(minutes=24 * 60)\u000a    def on_start(self):\u000a        self.crawl('http://www.qiushibaike.com', callback=self.index_page)\u000a\u000a    @config(age=10 * 24 * 60 * 60)\u000a    def index_page(self, response):\u000a        for each in response.doc('a[href^="http"]').items():\u000a            self.crawl(each.attr.href, callback=self.detail_page)\u000a\u000a    @config(priority=2)\u000a    def detail_page(self, response):\u000a        return {\u000a            "url": response.url,\u000a            "title": response.doc('title').text(),\u000a        }\u000a
p32
sS'burst'
p33
I3
sS'comments'
p34
NsS'rate'
p35
I1
ssS'_send_finished_event'
p36
I01
sS'md5sum'
p37
S'a50cfb82e8a757c1e0fe20c28548636b'
p38
sS'crawl_config'
p39
(dp40
sS'task_loaded'
p41
I01
sS'paused'
p42
I00
sS'retry_delay'
p43
(dp44
sS'_send_on_get_info'
p45
I00
sS'task_queue'
p46
g4
(cpyspider.scheduler.task_queue
TaskQueue
p47
g6
NtRp48
(dp49
S'bucket'
p50
g4
(cpyspider.scheduler.token_bucket
Bucket
p51
g6
NtRp52
(dp53
g50
I3
sS'rate'
p54
I1
sS'last_update'
p55
F1471690683.5050001
sS'mutex'
p56
